{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM52t/iyugEcEtSYl/ZrKYy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"xcU24-hG7M_T","executionInfo":{"status":"ok","timestamp":1714714138462,"user_tz":-330,"elapsed":10951,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}}},"outputs":[],"source":["import nltk #nltk stands for natural language toolkit used for natural language processing tasks\n","from nltk.tokenize import word_tokenize\n","import spacy #Spacy is a natural language processing (NLP) library for Python. It is used to build information extraction or natural language understanding systems, or to pre-process text for deep learning."]},{"cell_type":"code","source":["nltk.download('punkt') # punkt is a pre-trained tokenizer, a data-driven sentence tokenizer that comes with NLTK. It is trained on large corpus of text to identify sentence boundaries.\n","nlp = spacy.load('en_core_web_sm') #en_core_web_sm is a small English pipeline trained on the CoNLL 2000 shared task."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WN7Kk9WZ7bVc","executionInfo":{"status":"ok","timestamp":1714714172625,"user_tz":-330,"elapsed":3380,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}},"outputId":"94723d98-21c7-4707-88c2-c7064e03620d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["text = \"Natural Language Processing is fascinating!\"\n","tokens_nltk = word_tokenize(text) # word_tokenize is a function that splits a string into a list of words.\n","tokens_spacy = [token.text for token in nlp(text)] # [token.text for token in nlp(text)] is a list comprehension that creates a list of the text of each token in the SpaCy object."],"metadata":{"id":"ZDQcNsdV7keK","executionInfo":{"status":"ok","timestamp":1714714204754,"user_tz":-330,"elapsed":687,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"NLTK tokens:\", tokens_nltk)\n","print(\"spaCy tokens:\", tokens_spacy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LkT40m4j7lLb","executionInfo":{"status":"ok","timestamp":1714714252352,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}},"outputId":"704b76a3-bf56-49e1-bb85-89101ad94c32"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n","spaCy tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords # stopwords is a set of commonly used words in English that are usually not useful for text analysis., Corpus is a collection of text documents."],"metadata":{"id":"3N9aazJk7yxP","executionInfo":{"status":"ok","timestamp":1714714261828,"user_tz":-330,"elapsed":519,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english')) # stopwords.words('english') returns a list of stopwords in English. and set is used to convert the list to a set."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ih_g_8xk7zJJ","executionInfo":{"status":"ok","timestamp":1714714266621,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}},"outputId":"ee98efe9-b161-4bcc-e4fc-d69687ad7113"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["filtered_tokens = [token for token in tokens_nltk if token.lower() not in\n","stop_words] # [token for token in tokens_nltk if token.lower() not in stop_words] is a list comprehension that creates a list of tokens that are not stopwords."],"metadata":{"id":"M3H-eSwe70bw","executionInfo":{"status":"ok","timestamp":1714714272523,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(\"Filtered tokens:\", filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dn7BpG0c7158","executionInfo":{"status":"ok","timestamp":1714714279262,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}},"outputId":"83f3aeec-8ea0-4f68-81d7-42bacb851152"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered tokens: ['Natural', 'Language', 'Processing', 'fascinating', '!']\n"]}]},{"cell_type":"code","source":["doc = nlp(text) # nlp(text) is a SpaCy object that represents the text as a collection of tokens.\n","pos_tags = [(token.text, token.pos_) for token in doc] # [(token.text, token.pos_) for token in doc] is a list comprehension that creates a list of tuples of the text and part-of-speech tag of each token in the SpaCy object."],"metadata":{"id":"3kKOaoOF73gE","executionInfo":{"status":"ok","timestamp":1714714287255,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print(\"POS tags:\", pos_tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7ZanIlD75e9","executionInfo":{"status":"ok","timestamp":1714714296322,"user_tz":-330,"elapsed":567,"user":{"displayName":"Sree datta","userId":"13840540978112129498"}},"outputId":"70a9bde3-32a4-47d4-9bfc-9817933c2e2b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["POS tags: [('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'PROPN'), ('is', 'AUX'), ('fascinating', 'ADJ'), ('!', 'PUNCT')]\n"]}]},{"cell_type":"markdown","source":["# End"],"metadata":{"id":"tts4r9Bv-7eT"}}]}